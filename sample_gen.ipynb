{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     WRONG_VERSION_NUMBER] wrong version number\n",
      "[nltk_data]     (_ssl.c:1129)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     WRONG_VERSION_NUMBER] wrong version number\n",
      "[nltk_data]     (_ssl.c:1129)>\n",
      "[nltk_data] Error loading wordnet: HTTP Error 404: Not Found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error loading wordnet: HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:57\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mcorpora/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m token)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'd:\\\\dev\\\\fix-your-summary\\\\venv\\\\nltk_data'\n    - 'd:\\\\dev\\\\fix-your-summary\\\\venv\\\\share\\\\nltk_data'\n    - 'd:\\\\dev\\\\fix-your-summary\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:60\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     nltk\u001b[39m.\u001b[39;49mdownload(token, quiet \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, raise_on_error \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[39m# Sometimes there are problems with the default index.xml URL. Then we will try this...\u001b[39;00m\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\nltk\\downloader.py:782\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m    783\u001b[0m \u001b[39mif\u001b[39;00m halt_on_error:\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: <urlopen error [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1129)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymbols\u001b[39;00m \u001b[39mimport\u001b[39;00m nsubj, VERB\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m \u001b[39mimport\u001b[39;00m conjugate, PAST, PRESENT, tenses, parse, pprint, parsetree, SINGULAR, PLURAL\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m tee\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\pattern\\text\\en\\__init__.py:61\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     INFINITIVE, PRESENT, PAST, FUTURE,\n\u001b[0;32m     55\u001b[0m     FIRST, SECOND, THIRD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     PARTICIPLE\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[39m# Import inflection functions.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minflect\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     article, referenced, DEFINITE, INDEFINITE,\n\u001b[0;32m     63\u001b[0m     pluralize, singularize, NOUN, VERB, ADJECTIVE,\n\u001b[0;32m     64\u001b[0m     grade, comparative, superlative, COMPARATIVE, SUPERLATIVE,\n\u001b[0;32m     65\u001b[0m     verbs, conjugate, lemma, lexeme, tenses,\n\u001b[0;32m     66\u001b[0m     predicative, attributive\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[39m# Import quantification functions.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minflect_quantify\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     70\u001b[0m     number, numerals, quantify, reflect\n\u001b[0;32m     71\u001b[0m )\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\pattern\\text\\en\\__init__.py:80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m# Import all submodules.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m \u001b[39mimport\u001b[39;00m inflect\n\u001b[1;32m---> 80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m \u001b[39mimport\u001b[39;00m wordnet\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpattern\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m \u001b[39mimport\u001b[39;00m wordlist\n\u001b[0;32m     83\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mimport\u001b[39;00m Downloader \u001b[39mas\u001b[39;00m NLTKDownloader\n\u001b[0;32m     64\u001b[0m             d \u001b[39m=\u001b[39m NLTKDownloader(\u001b[39m\"\u001b[39m\u001b[39mhttp://nltk.github.com/nltk_data/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m             d\u001b[39m.\u001b[39;49mdownload(token, quiet \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, raise_on_error \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     67\u001b[0m \u001b[39m# Use the Brown corpus for calculating information content (IC)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m brown_ic \u001b[39m=\u001b[39m wn_ic\u001b[39m.\u001b[39mic(\u001b[39m'\u001b[39m\u001b[39mic-brown.dat\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\dev\\fix-your-summary\\venv\\lib\\site-packages\\nltk\\downloader.py:782\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    780\u001b[0m show(msg\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m    783\u001b[0m \u001b[39mif\u001b[39;00m halt_on_error:\n\u001b[0;32m    784\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "from spacy.symbols import nsubj, VERB\n",
    "from pattern.en import conjugate, PAST, PRESENT, tenses, parse, pprint, parsetree, SINGULAR, PLURAL\n",
    "from itertools import tee\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker_names(data_list):\n",
    "    speaker_names = set()\n",
    "    for data in data_list:\n",
    "        dialogue = data[\"dialogue\"]\n",
    "        for line in dialogue.split(\"\\n\"):\n",
    "            speaker = line.split(\":\")[0].strip()\n",
    "            if speaker:\n",
    "                speaker_names.add(speaker)\n",
    "    return list(speaker_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names = extract_speaker_names(dataset[\"train\"])\n",
    "pronouns = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"them\", \"his\", \"hers\", \"its\", \"theirs\"]\n",
    "tenses = [\"past\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(labels, label_type, start_idx, end_idx, new_token):\n",
    "    new_labels = []\n",
    "    token_length = len(new_token.split())\n",
    "    for i in range(len(labels)):\n",
    "        if i < start_idx:\n",
    "            new_labels.append(labels[i])\n",
    "        elif i == start_idx:\n",
    "            for j in range(token_length):\n",
    "                new_labels.append(label_type)\n",
    "        elif i > start_idx and i < end_idx:\n",
    "            continue\n",
    "        else:\n",
    "            new_labels.append(labels[i])\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_named_person(summary, dialog, labels):\n",
    "    dialog_doc = nlp(dialog)\n",
    "    dialog_entities = [ent.text for ent in dialog_doc.ents if ent.label_ in [\"PERSON\"]]\n",
    "    \n",
    "    summary_doc = nlp(summary)\n",
    "    summary_entities = [ent for ent in summary_doc.ents if ent.label_ in [\"PERSON\"]]\n",
    "    \n",
    "    if len(summary_entities) == 0:\n",
    "        return summary\n",
    "    \n",
    "    for entity in reversed(summary_entities):\n",
    "        if random.random() < 0.5:\n",
    "            continue  # Skip replacing this entity with a new name\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            new_name = random.choice(speaker_names)\n",
    "            while new_name == entity.text:\n",
    "                new_name = random.choice(speaker_names)\n",
    "        else:\n",
    "            new_name = random.choice(dialog_entities)\n",
    "            while new_name == entity.text:\n",
    "                new_name = random.choice(dialog_entities)\n",
    "        \n",
    "        labels = update_labels(labels, \"E\", entity.start, entity.end, new_name)\n",
    "        summary = summary[:entity.start_char] + new_name + summary[entity.end_char:]\n",
    "    \n",
    "    return summary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pronoun(summary, labels):\n",
    "    summary_doc = nlp(summary)\n",
    "    for token in reversed(summary_doc):\n",
    "        if token.pos_ == \"PRON\":\n",
    "            pronoun = token\n",
    "            if random.random() < 0.5:\n",
    "                continue  # Skip replacing this pronoun\n",
    "            new_pronoun = random.choice(pronouns)\n",
    "            while new_pronoun == pronoun.text:\n",
    "                new_pronoun = random.choice(pronouns)\n",
    "            \n",
    "            labels = update_labels(labels, \"P\", pronoun.i, pronoun.i + 1, new_pronoun)\n",
    "            summary = summary[:pronoun.idx] + new_pronoun + summary[pronoun.idx + len(pronoun):]\n",
    "    \n",
    "    return summary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_verb(summary, dialog, labels):\n",
    "    # Extract verbs from the dialog\n",
    "    dialog_doc = nlp(dialog)\n",
    "    dialog_verbs = [token.lemma_ for token in dialog_doc if token.pos_ == 'VERB']\n",
    "\n",
    "    # Replace verbs in the summary\n",
    "    summary_doc = nlp(summary)\n",
    "    for token in reversed(summary_doc):\n",
    "        if token.pos_ == 'VERB':\n",
    "            verb = token\n",
    "            new_verb = random.choice(dialog_verbs)\n",
    "            while new_verb == verb.text:\n",
    "                new_verb = random.choice(dialog_verbs)\n",
    "            \n",
    "            labels = update_labels(labels, \"V\", verb.i, verb.i + 1, new_verb)\n",
    "            summary = summary[:verb.idx] + new_verb + summary[verb.idx + len(verb):]\n",
    "\n",
    "    return summary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_tense_spaCy(summary, to_tense):\n",
    "    \n",
    "    doc = nlp(unicode(text))\n",
    "    \n",
    "    out = []\n",
    "    out.append(doc[0].text)\n",
    "    for word_pair in pairwise(doc):\n",
    "        if (word_pair[0].string == 'will' and word_pair[1].pos_ == u'VERB') \\\n",
    "        or word_pair[1].tag_ == u'VBD' or word_pair[1].tag_ == u'VBP':\n",
    "            if to_tense == 'present':\n",
    "                out.append(conjugate(word_pair[1].text, PRESENT))\n",
    "            elif to_tense == 'past':\n",
    "                out.append(conjugate(word_pair[1].text, PAST))\n",
    "            elif to_tense == 'future':\n",
    "                out.append('will')\n",
    "                out.append(conjugate(word_pair[1].text, 'inf'))\n",
    "\n",
    "            elif word_pair[1].text == 'will' and word_pair[1].tag_ == 'MD':\n",
    "                pass\n",
    "        else:\n",
    "            out.append(word_pair[1].text)\n",
    "\n",
    "\n",
    "    text_out = ' '.join(out)\n",
    "\n",
    "    for char in string.punctuation:\n",
    "        if char in \"\"\"(<['â€˜\"\"\":\n",
    "            text_out = text_out.replace(char+' ',char)\n",
    "        else:\n",
    "            text_out = text_out.replace(' '+char,char)\n",
    "\n",
    "    text_out = text_out.replace(\" 's\",\"'s\") #fix posessive 's\n",
    "    \n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Amelia bake cookies and will bake Jerry some tomorrow . He is really happy now\n",
      "15\n",
      "['E', 'V', 'O', 'O', 'O', 'V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "data = dataset[\"train\"][0]\n",
    "summary_doc = nlp(\"Michael Oliver baked cookies and will bring Jerry some tomorrow . He is really happy now\")\n",
    "summary = \" \".join([token.text for token in summary_doc])\n",
    "labels = [\"O\"] * len(summary.split())\n",
    "distorted, labels = replace_named_person(summary, data[\"dialogue\"], labels)\n",
    "print(labels)\n",
    "distorted, labels = replace_pronoun(distorted, labels)\n",
    "distorted, labels = replace_verb(distorted, data[\"dialogue\"], labels)\n",
    "# distorted = \"Michael Oliver baked cookies and will bring Jerry some tomorrow.\"\n",
    "print(distorted)\n",
    "print(len(distorted.split()))\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "# print(distorted[14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Michael PROPN NNP compound Xxxxx True False\n",
      "Oliver Oliver PROPN NNP nsubj Xxxxx True False\n",
      "give give VERB VB ROOT xxxx True True\n",
      "back back ADP RP prt xxxx True True\n",
      "cookies cookie NOUN NNS dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "will will AUX MD aux xxxx True True\n",
      "bring bring VERB VB conj xxxx True False\n",
      "Jerry Jerry PROPN NNP dative Xxxxx True False\n",
      "some some DET DT det xxxx True True\n",
      "tomorrow tomorrow NOUN NN npadvmod xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "He he PRON PRP nsubj Xx True True\n",
      "is be AUX VBZ ROOT xx True True\n",
      "really really ADV RB advmod xxxx True True\n",
      "happy happy ADJ JJ acomp xxxx True False\n",
      "now now ADV RB advmod xxx True True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Oliver give back cookies and will bring Jerry some tomorrow . He is really happy now\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"AutoModelForSeq2SeqLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
