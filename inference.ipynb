{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.helper import *\n",
    "from utils.labels_tags import *\n",
    "from utils.correction_ratio import *\n",
    "\n",
    "from utils.helper import nlp\n",
    "from utils.labels_tags import label_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute metric code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haznitrama/fix-your-summary/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 6.27kB [00:00, 5.34MB/s]\n",
      "Downloading builder script: 7.95kB [00:00, 7.37MB/s]\n",
      "Downloading: 899kB [00:00, 1.73MB/s]\n",
      "Downloading: 456kB [00:00, 1.27MB/s]\n",
      "Downloading: 1.36MB [00:00, 2.44MB/s]\n",
      "Downloading: 1.58kB [00:00, 1.81MB/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from bart_score import BARTScorer\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics_inference(preds, targets):\n",
    "    # Some simple post-processing\n",
    "    preds, targets = postprocess_text(preds, targets)\n",
    "\n",
    "    bart_score_p = mean(bart_scorer.score(targets, preds, batch_size=4))\n",
    "    bart_score_r = mean(bart_scorer.score(preds, targets, batch_size=4))\n",
    "    bart_score_f = 0.5 * (bart_score_p + bart_score_r)\n",
    "    bart_score_result = {\n",
    "        \"precision\": bart_score_p,\n",
    "        \"recall\": bart_score_r,\n",
    "        \"f_score\": bart_score_f,\n",
    "    }\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=preds, references=targets, use_stemmer=True)\n",
    "    rouge_result = {key: value * 100 for key, value in rouge_result.items()}\n",
    "    rouge_result = {k: round(v, 4) for k, v in rouge_result.items()}\n",
    "\n",
    "    bs_result = bertscore.compute(predictions=preds, references=targets, lang=\"en\")\n",
    "    bs_result = {key: np.mean(value) * 100 for key, value in bs_result.items() if key != \"hashcode\"}\n",
    "    bs_result = {k: round(v, 4) for k, v in bs_result.items()}\n",
    "\n",
    "    return {\n",
    "        \"bart_score\": bart_score_result,\n",
    "        \"rouge\": rouge_result,\n",
    "        \"bert-score\": bs_result,\n",
    "    }\n",
    "\n",
    "def compute_metrics_test(preds, targets, dataset):\n",
    "    # Some simple post-processing\n",
    "    preds, targets = postprocess_text(preds, targets)\n",
    "\n",
    "    num_dist_hard = 0\n",
    "    num_cor_hard = 0\n",
    "    num_dist_soft = 0\n",
    "    num_cor_soft = 0\n",
    "    for idx, d in enumerate(dataset):\n",
    "        cor_hard, dist_hard = compute_correction_ratio(d, preds[idx])\n",
    "        num_cor_hard += cor_hard\n",
    "        num_dist_hard += dist_hard\n",
    "        cor_soft, dist_soft = compute_correction_ratio(d, preds[idx], soft=True)\n",
    "        num_cor_soft += cor_soft\n",
    "        num_dist_soft += dist_soft\n",
    "\n",
    "    corr_result = {\n",
    "        \"corr_score_hard\": num_cor_hard / num_dist_hard,\n",
    "        \"corr_score_soft\": num_cor_soft / num_dist_soft,\n",
    "    }\n",
    "\n",
    "    bart_score_p = mean(bart_scorer.score(targets, preds, batch_size=4))\n",
    "    bart_score_r = mean(bart_scorer.score(preds, targets, batch_size=4))\n",
    "    bart_score_f = 0.5 * (bart_score_p + bart_score_r)\n",
    "    bart_score_result = {\n",
    "        \"precision\": bart_score_p,\n",
    "        \"recall\": bart_score_r,\n",
    "        \"f_score\": bart_score_f,\n",
    "    }\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=preds, references=targets, use_stemmer=True)\n",
    "    rouge_result = {key: value * 100 for key, value in rouge_result.items()}\n",
    "    rouge_result = {k: round(v, 4) for k, v in rouge_result.items()}\n",
    "\n",
    "    bs_result = bertscore.compute(predictions=preds, references=targets, lang=\"en\")\n",
    "    bs_result = {key: np.mean(value) * 100 for key, value in bs_result.items() if key != \"hashcode\"}\n",
    "    bs_result = {k: round(v, 4) for k, v in bs_result.items()}\n",
    "\n",
    "    return {\n",
    "        \"correction_ratio\": corr_result,\n",
    "        \"bart_score\": bart_score_result,\n",
    "        \"rouge\": rouge_result,\n",
    "        \"bert-score\": bs_result,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize joint bert model and funct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "sys.argv=['']\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1234, help=\"random seed for initialization\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_batch_size\", default=8, type=int, help=\"Batch size for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_batch_size\", default=16, type=int, help=\"Batch size for evaluation.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_len\",\n",
    "    default=512,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after tokenization.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    default=2e-5,\n",
    "    type=float,\n",
    "    help=\"The initial learning rate for Adam.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_train_epochs\",\n",
    "    default=10.0,\n",
    "    type=float,\n",
    "    help=\"Total number of training epochs to perform.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",\n",
    "    default=0.01,\n",
    "    type=float,\n",
    "    help=\"Weight decay if we apply some.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_steps\",\n",
    "    default=-1,\n",
    "    type=int,\n",
    "    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dropout_rate\",\n",
    "    default=0.1,\n",
    "    type=float,\n",
    "    help=\"Dropout for fully-connected layers\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_steps\",\n",
    "    type=int,\n",
    "    default=500,\n",
    "    help=\"Save checkpoint every X updates steps.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--do_train\", action=\"store_true\", help=\"Whether to run training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_predict\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to run predict on the test set.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--ignore_index\",\n",
    "    default=-100,\n",
    "    type=int,\n",
    "    help=\"Specifies a target value that is ignored and does not contribute to the input gradient\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--slot_loss_coef\",\n",
    "    type=float,\n",
    "    default=1.0,\n",
    "    help=\"Coefficient for the slot loss.\",\n",
    ")\n",
    "\n",
    "# CRF option\n",
    "parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "parser.add_argument(\n",
    "    \"--slot_pad_label\",\n",
    "    default=\"PAD\",\n",
    "    type=str,\n",
    "    help=\"Pad token for slot label pad (to be ignore when calculate loss)\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default=\"bert-large-uncased\",\n",
    "    type=str,\n",
    "    help=\"Model for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_dir\", default=\"./data/dialogsum/\", type=str, help=\"Input data dir\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--label_type\", default=\"labels_bio\", type=str, help=\"Label type\"\n",
    ")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids, context_len):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for i in range(len(word_ids)):\n",
    "        if i < context_len + 2:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            if word_ids[i] != current_word:\n",
    "                # Start of a new word!\n",
    "                current_word = word_ids[i]\n",
    "                label = -100 if word_ids[i] is None else labels[word_ids[i]]\n",
    "                new_labels.append(label)\n",
    "            else:\n",
    "                # Special token or same word as prev. token\n",
    "                new_labels.append(-100)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def joint_model_inference(dialog, draft):\n",
    "    tokenized_inputs = joint_tokenizer(\n",
    "        dialog,\n",
    "        draft,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    dummy_labels = [0] * len(draft)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    context_len = len(joint_tokenizer.tokenize(dialog, is_split_into_words=True))\n",
    "    tokenized_inputs[\"labels\"] = align_labels_with_tokens(dummy_labels, word_ids, context_len)\n",
    "    tokenized_inputs[\"hallucination_label_ids\"] = 0\n",
    "\n",
    "    joint_model.eval()\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor([tokenized_inputs[\"input_ids\"]], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([tokenized_inputs[\"attention_mask\"]], dtype=torch.long),\n",
    "        \"token_type_ids\": torch.tensor([tokenized_inputs[\"token_type_ids\"]], dtype=torch.long),\n",
    "        \"intent_label_ids\": torch.tensor([tokenized_inputs[\"hallucination_label_ids\"]], dtype=torch.long),\n",
    "        \"slot_labels_ids\": torch.tensor([tokenized_inputs[\"labels\"]], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "    outputs = joint_model(**inputs)\n",
    "    _, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "    intent_preds = intent_logits.detach().cpu().numpy()\n",
    "    intent_preds = np.argmax(intent_preds, axis=1)\n",
    "\n",
    "    slot_preds = slot_logits.detach().cpu().numpy()\n",
    "    slot_preds = np.argmax(slot_preds, axis=2)\n",
    "    out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
    "    slot_label_map = {i: label for i, label in enumerate(label_map[args.label_type][\"tags\"])}\n",
    "    out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "    slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_slot_labels_ids.shape[0]):\n",
    "        for j in range(out_slot_labels_ids.shape[1]):\n",
    "            if out_slot_labels_ids[i, j] != args.ignore_index:\n",
    "                out_slot_label_list[i].append(\n",
    "                    slot_label_map[out_slot_labels_ids[i][j]]\n",
    "                )\n",
    "                slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "    \n",
    "    return intent_preds[0], slot_preds_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corrector pipeline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier_prediction(identifier, dialog, draft_tokens):\n",
    "    encoded = cased_tokenizer(draft_tokens, is_split_into_words=True)\n",
    "    word_ids = encoded.word_ids()\n",
    "    word_ids = word_ids[1:-1]\n",
    "    missing_nums = find_missing_numbers(word_ids)\n",
    "\n",
    "    if \"joint\" in identifier:\n",
    "        hallucinated_pred, pred_labels = joint_model_inference(dialog, draft_tokens)\n",
    "        if missing_nums:\n",
    "            for i in missing_nums:\n",
    "                pred_labels.insert(i, \"B-E\")\n",
    "        is_hallucinated = bool(hallucinated_pred) or not all(l == \"O\" for l in pred_labels)\n",
    "    else:\n",
    "        tokenized_inputs = tokenizer(dialog, draft_tokens, is_split_into_words=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = token_classifier(**tokenized_inputs).logits\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        predicted_token_class = [token_classifier.config.id2label[t.item()] for t in predictions[0]]\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids()\n",
    "        first_idx = (tokenized_inputs.token_type_ids[0] == 1).nonzero(as_tuple=True)[0][0]\n",
    "        word_ids = word_ids[first_idx:-1]\n",
    "        raw_preds = predicted_token_class[first_idx:-1]\n",
    "\n",
    "        pred_map = map_predictions(raw_preds, word_ids)\n",
    "        if missing_nums:\n",
    "            for i in missing_nums:\n",
    "                if i in pred_map.keys():\n",
    "                    pred_map[i].add(\"B-E\")\n",
    "                else:\n",
    "                    pred_map[i] = {\"B-E\"}\n",
    "\n",
    "        pred_labels = align_pred_labels(pred_map, draft_tokens)\n",
    "        is_hallucinated = not all(l == \"O\" for l in pred_labels)\n",
    "    \n",
    "    return pred_labels, is_hallucinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_corrector_pipeline(dataset, mode=\"full\", identifier=\"joint\", supervision=\"tag\", model_type=\"proposed\", iterative=False, max_iter=5, prompt=None, draft_summaries=None):\n",
    "    assert mode in [\"full\", \"distort_only\"]\n",
    "    assert identifier in [\"joint\", \"joint_comb\", \"token_cls\", \"token_cls_comb\", \"bart\", \"ideal\"]\n",
    "    assert supervision in [\"tag\", \"list\"]\n",
    "    assert model_type in [\"baseline\", \"proposed\"]\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    count_corrected = 0\n",
    "    \n",
    "    corrected_summaries = []\n",
    "    if mode == \"distort_only\" and draft_summaries is None:\n",
    "        dataset = [d for d in dataset if not all(l == 0 for l in d[label_type])]\n",
    "\n",
    "    if draft_summaries is not None:\n",
    "        targets = [d[\"summary\"] for d in dataset]\n",
    "    else:\n",
    "        targets = [d[\"ref_summaries\"] for d in dataset]\n",
    "    \n",
    "    for idx, d in tqdm(enumerate(dataset)):\n",
    "        if draft_summaries is not None:\n",
    "            assert len(draft_summaries) == len(dataset)\n",
    "            assert mode == \"full\"\n",
    "            \n",
    "            dialog = d[\"dialogue\"]\n",
    "            dialog_tokens = [token.text for token in nlp(dialog)]\n",
    "            draft = draft_summaries[idx]\n",
    "            raw_draft = tokenize_summary(draft)\n",
    "            while draft != raw_draft:\n",
    "                draft = raw_draft\n",
    "                raw_draft = tokenize_summary(draft)\n",
    "            draft = raw_draft\n",
    "            draft_tokens = draft.split()\n",
    "        else:\n",
    "            dialog = \" \".join(d[\"dialogues\"])\n",
    "            dialog_tokens = d[\"dialogues\"]\n",
    "            draft = \" \".join(d[\"distorted_summaries\"])\n",
    "            draft_tokens = d[\"distorted_summaries\"]\n",
    "\n",
    "        iteration = 1\n",
    "        while iteration <= max_iter:\n",
    "            if identifier == \"ideal\" and draft_summaries is None:\n",
    "                pred_labels = d[f\"tag_{label_type}\"]\n",
    "                is_hallucinated = not all(l == \"O\" for l in pred_labels)\n",
    "                if iteration > 1:\n",
    "                    pred_labels = [\"O\"] * len(draft_tokens)\n",
    "                    is_hallucinated = False\n",
    "            elif identifier == \"bart\":\n",
    "                pred_labels = [\"O\"] * len(draft_tokens)\n",
    "                identifier_input = f\"{dialog} </s></s> {draft}\"\n",
    "                input_ids = bart_identifier_tokenizer(identifier_input, max_length=1024, truncation=True, return_tensors='pt')['input_ids']\n",
    "                summary_ids = bart_identifier.generate(\n",
    "                            input_ids.to(torch.device(\"cuda:0\")),\n",
    "                            max_length=128,\n",
    "                            min_length=5,\n",
    "                            num_beams=6, \n",
    "                            no_repeat_ngram_size=3)\n",
    "                identifier_result = bart_identifier_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                if supervision == \"tag\":\n",
    "                    tagged_summary = identifier_result\n",
    "                    if \"<\" in tagged_summary and \">\" in tagged_summary:\n",
    "                        is_hallucinated = True\n",
    "                    else:\n",
    "                        is_hallucinated = False\n",
    "                elif supervision == \"list\":\n",
    "                    word_list = identifier_result\n",
    "                    is_hallucinated = any(char.isalpha() for char in word_list)\n",
    "            else:\n",
    "                pred_labels, is_hallucinated = get_identifier_prediction(identifier, dialog_tokens, draft_tokens)\n",
    "            \n",
    "            if draft_summaries is None:\n",
    "                assert len(pred_labels) == len(draft_tokens)\n",
    "\n",
    "            uncorrected_summary = cased_tokenizer.decode(cased_tokenizer(draft)[\"input_ids\"][1:-1])\n",
    "\n",
    "            if not is_hallucinated:\n",
    "                if iteration == 1:\n",
    "                    if draft_summaries is None:\n",
    "                        if all(l == 0 for l in d[label_type]):\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                        corrected_summaries.append(uncorrected_summary)\n",
    "                    else:\n",
    "                        corrected_summaries.append(draft_summaries[idx])\n",
    "                else:\n",
    "                    corrected_summaries.append(uncorrected_summary)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                label2id = label_map[label_type][\"label2id\"]\n",
    "                pred_labels = [label2id[label] for label in pred_labels]\n",
    "\n",
    "                if model_type == \"baseline\":\n",
    "                    input_str = f\"{dialog} </s></s> {draft}\"\n",
    "                elif model_type == \"proposed\":\n",
    "                    if supervision == \"tag\":\n",
    "                        if \"bio\" in label_type:\n",
    "                            corrector_input = add_tags_to_summary_bio(draft_tokens, pred_labels, label_type)\n",
    "                        else:\n",
    "                            corrector_input = add_tags_to_summary(draft_tokens, pred_labels, label_type)\n",
    "                        \n",
    "                        if identifier == \"bart\":\n",
    "                            corrector_input = tagged_summary\n",
    "                            \n",
    "                        input_str = f\"{dialog} </s></s> {corrector_input}\"\n",
    "                    else:\n",
    "                        if identifier != \"bart\":\n",
    "                            word_list = get_hallucinated_word_list(pred_labels, draft_tokens, label_type)\n",
    "                        if prompt:\n",
    "                            input_str = f\"{prompt} </s></s> Word List: {word_list} </s></s> Draft Summary: {draft} </s></s> Dialogue Context: {dialog}\"\n",
    "                        else:\n",
    "                            raise ValueError(\"List Supervision need prompt\")\n",
    "\n",
    "                input_ids = corrector_tokenizer(input_str, max_length=1024, truncation=True, return_tensors='pt')['input_ids']\n",
    "                summary_ids = corrector.generate(\n",
    "                            input_ids.to(torch.device(\"cuda:0\")),\n",
    "                            max_length=128,\n",
    "                            min_length=5,\n",
    "                            num_beams=6, \n",
    "                            no_repeat_ngram_size=3)\n",
    "                corrected_summary = corrector_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                is_same = compare_strings(corrected_summary, uncorrected_summary)\n",
    "\n",
    "                if iteration == 1:\n",
    "                    if is_same:\n",
    "                        if draft_summaries is None:\n",
    "                            if all(l == 0 for l in d[label_type]):\n",
    "                                TN += 1\n",
    "                            else:\n",
    "                                FN += 1\n",
    "                    else:\n",
    "                        if draft_summaries is None:\n",
    "                            if not all(l == 0 for l in d[label_type]):\n",
    "                                TP += 1\n",
    "                            else:\n",
    "                                FP += 1\n",
    "                        else:\n",
    "                            count_corrected += 1\n",
    "\n",
    "                if not iterative or iteration == max_iter or is_same:\n",
    "                    if iteration == 1 and is_same:\n",
    "                        if draft_summaries is None:\n",
    "                            corrected_summaries.append(uncorrected_summary)\n",
    "                        else:\n",
    "                            corrected_summaries.append(draft_summaries[idx])\n",
    "                    else:\n",
    "                        corrected_summaries.append(corrected_summary)\n",
    "                    break\n",
    "                else:\n",
    "                    draft = corrected_summary.strip()\n",
    "                    raw_draft = tokenize_summary(draft)\n",
    "                    while draft != raw_draft:\n",
    "                        draft = raw_draft\n",
    "                        raw_draft = tokenize_summary(draft)\n",
    "                    draft = raw_draft\n",
    "                    draft_tokens = draft.split()\n",
    "                    iteration += 1\n",
    "\n",
    "    output_filename = f\"test_{identifier}_{mode}_{supervision}_predict\"\n",
    "    if iterative:\n",
    "        output_filename += f\"_iter{max_iter}\"\n",
    "    \n",
    "    if draft_summaries is not None:\n",
    "        output_filename = \"inference_\" + output_filename\n",
    "\n",
    "    if draft_summaries is None:\n",
    "        result = compute_metrics_test(corrected_summaries, targets, dataset)\n",
    "        hallucination_pred_acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        tpr = TP / (TP + FN)\n",
    "        tnr = TN / (TN + FP)\n",
    "        result[\"pred_acc\"] = hallucination_pred_acc\n",
    "        result[\"TPR\"] = tpr\n",
    "        result[\"TNR\"] = tnr\n",
    "        result[\"balanced_acc\"] = (tpr + tnr) / 2\n",
    "        result[\"TP\"] = TP\n",
    "        result[\"TN\"] = TN\n",
    "        result[\"FP\"] = FP\n",
    "        result[\"FN\"] = FN\n",
    "    else:\n",
    "        result = compute_metrics_inference(corrected_summaries, targets)\n",
    "        result[\"corrected_summary\"] = count_corrected\n",
    "\n",
    "    with open(f\"{corrector_dir}/{output_filename}.txt\", 'w') as f:\n",
    "        f.write(\"\\n\".join(corrected_summaries))\n",
    "\n",
    "    with open(f\"{corrector_dir}/{output_filename}.json\", 'w') as f:\n",
    "        json.dump(result, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data_dir = \"./data/dialogsum\"\n",
    "dataset = load_from_disk(data_dir)\n",
    "\n",
    "with open(\"./model/dialogsum/vanilla/bart-base/test_predict.txt\") as f:\n",
    "    draft_summaries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_jointbert import JointBERT\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForTokenClassification, BertConfig, AutoTokenizer\n",
    "\n",
    "label_type = \"labels_sep_bio\"\n",
    "args.label_type = label_type\n",
    "model_type = label_type\n",
    "\n",
    "cased_checkpoint = \"bert-base-cased\"\n",
    "\n",
    "tokencls_dir = f\"./model/dialogsum/span-predictor/bert-large-uncased-x0.5-alpha0.5-lfp0.3-insertion-{label_type}\"\n",
    "joint_dir = f\"./model/dialogsum/joint-predictor/bert-large-uncased-x0.5-alpha0.5-lfp0.3-insertion-{label_type}\"\n",
    "bart_identifier_dir = f\"./model/dialogsum/identifier/bart-large-x0.5-alpha0.5-lfp0.3-insertion-identifier-sep\"\n",
    "corrector_dir = f\"./model/dialogsum/baseline/baseline-bart-large-x0.5-alpha0.5-lfp0.3-insertion-all\"\n",
    "\n",
    "args.model_dir = joint_dir\n",
    "\n",
    "joint_tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "joint_config = BertConfig.from_pretrained(args.model_dir)\n",
    "joint_model = JointBERT.from_pretrained(\n",
    "    args.model_dir,\n",
    "    config=joint_config,\n",
    "    args=args,\n",
    "    intent_label_lst=[\"clean\", \"distorted\"],\n",
    "    slot_label_lst=label_map[args.label_type][\"tags\"],\n",
    ")\n",
    "\n",
    "cased_tokenizer = AutoTokenizer.from_pretrained(cased_checkpoint, use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokencls_dir, use_fast=True)\n",
    "token_classifier = AutoModelForTokenClassification.from_pretrained(tokencls_dir)\n",
    "\n",
    "bart_identifier_tokenizer = AutoTokenizer.from_pretrained(bart_identifier_dir)\n",
    "bart_identifier = AutoModelForSeq2SeqLM.from_pretrained(bart_identifier_dir).to(torch.device(\"cuda:0\"))\n",
    "\n",
    "corrector_tokenizer = AutoTokenizer.from_pretrained(corrector_dir)\n",
    "corrector = AutoModelForSeq2SeqLM.from_pretrained(corrector_dir).to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [01:07,  4.04it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "1500it [06:32,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/23/2023 22:19:40 - INFO - absl - Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "list_prompt = \"Given a dialogue context, a draft summary, a list of potential factually incorrect words/spans, produce a faithful final summary based on the dialogue context.\"\n",
    "\n",
    "evaluate_corrector_pipeline(dataset[\"test\"], mode=\"full\", identifier=\"joint\", supervision=\"tag\", model_type=\"baseline\", iterative=False, prompt=None, draft_summaries=draft_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
